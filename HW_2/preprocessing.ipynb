{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.json.zip           processed_corpus.json.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/corpus.json') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Текстов - 1157366, слов - 16028874'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Текстов - {}, слов - {}'.format(len(corpus), sum([len(sample) for sample in corpus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    \n",
    "    word_data = morph.parse(word)[0]\n",
    "    \n",
    "    return word_data.normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соберем словарь лем\n",
    "В нашем корпусе 16028874 слов. Лемматизировать весь корпус будет очень долго. Давайте лучше соберем словарь уникальных слов и будет лемматизировать только уникальные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:42<00:00, 27528.92it/s]\n"
     ]
    }
   ],
   "source": [
    "tok2lemma = {}\n",
    "\n",
    "for text in tqdm(corpus):\n",
    "    for tok in text:\n",
    "        if tok not in tok2lemma:\n",
    "            tok2lemma[tok] = get_lemma(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'уехать'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ключ - уникальное слово в нашем корпусе, значение - его лемма\n",
    "tok2lemma['уехали']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193435"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok2lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Замена слов леммами\n",
    "Так как теперь к каждому слову из нашего корпуса мы знаем лемму, то давайте каждое слово заменим на его лемму и уберем стоп слова.\n",
    "Это работает гораздо(!) быстрее, чем если бы мы в корпусе для каждого слова каждый раз рассчитывали лемму (с помощью пайморфи), \n",
    "потому что теперь нам надо вызвать пайморфи 193435 раз вместо 16028874."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:29<00:00, 39416.01it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmas_corpus = [[tok2lemma[tok] for tok in text if tok not in stopwords and tok]\n",
    "                 for text in tqdm(corpus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соберем частотный словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:03<00:00, 345979.15it/s]\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "\n",
    "for text in tqdm(lemmas_corpus):\n",
    "    for tok in text:\n",
    "        if tok in freq:\n",
    "            freq[tok] += 1\n",
    "        else:\n",
    "            freq[tok] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.DataFrame(data={'word': list(freq.keys()), 'n_entries': list(freq.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.sort_values(by=['n_entries'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n_entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>##число</td>\n",
       "      <td>413016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>банк</td>\n",
       "      <td>333814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>карта</td>\n",
       "      <td>156216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>кредит</td>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>это</td>\n",
       "      <td>81577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  n_entries\n",
       "21   ##число     413016\n",
       "3       банк     333814\n",
       "46     карта     156216\n",
       "134   кредит      86865\n",
       "100      это      81577"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76916, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# уникальных слов в словаре\n",
    "freq_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n_entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21814</th>\n",
       "      <td>приборный</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21813</th>\n",
       "      <td>подписнуть</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21812</th>\n",
       "      <td>позволинь</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50667</th>\n",
       "      <td>marinaurievnaобещание</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76915</th>\n",
       "      <td>ситуевина</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        word  n_entries\n",
       "21814              приборный          1\n",
       "21813             подписнуть          1\n",
       "21812              позволинь          1\n",
       "50667  marinaurievnaобещание          1\n",
       "76915              ситуевина          1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = freq_df.n_entries.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Замена редких слов\n",
    "В нашем корпусе осталось много слов, которые встречаются очень редко. Давайте мы редкие слова заменим на специальный токе UNK - unknown. Так мы разительно сократим размер нашего словаря слов с незначительной потерей информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля слов, которые мы заменим на UNK:\n",
      "Порог отсечения - 5, доля UNK - 0.76 %, слов в слове - 23307, удалили - 53609 слов\n",
      "Порог отсечения - 10, доля UNK - 1.18 %, слов в слове - 16771, удалили - 60145 слов\n",
      "Порог отсечения - 15, доля UNK - 1.51 %, слов в слове - 13782, удалили - 63134 слов\n",
      "Порог отсечения - 20, доля UNK - 1.81 %, слов в слове - 11946, удалили - 64970 слов\n",
      "Порог отсечения - 25, доля UNK - 2.07 %, слов в слове - 10737, удалили - 66179 слов\n",
      "Порог отсечения - 30, доля UNK - 2.30 %, слов в слове - 9835, удалили - 67081 слов\n",
      "Порог отсечения - 35, доля UNK - 2.52 %, слов в слове - 9135, удалили - 67781 слов\n"
     ]
    }
   ],
   "source": [
    "print('Доля слов, которые мы заменим на UNK:')\n",
    "\n",
    "for threshold in np.arange(5, 36, 5):\n",
    "    \n",
    "    sub_df = freq_df[freq_df.n_entries < threshold]\n",
    "    \n",
    "    unk_freq = sub_df['n_entries'].sum() * 100 / n_words\n",
    "    \n",
    "    print('Порог отсечения - {}, доля UNK - {:.2f} %, слов в слове - {}, удалили - {} слов'.format(\n",
    "        threshold, unk_freq, freq_df.shape[0] - sub_df.shape[0], sub_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кажется, что оптимально, но обычно берут меньше\n",
    "threshold = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = freq_df[freq_df.n_entries >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(vocab.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13782"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мы сократили наш словарь в 5.58 раз с потерей 1.51 % всех слов'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Мы сократили наш словарь в {:.2f} раз с потерей 1.51 % всех слов'.format(freq_df.shape[0] / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_words(word):\n",
    "    \n",
    "    if word in words:\n",
    "        return word\n",
    "    else:\n",
    "        return 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:04<00:00, 260192.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# заменим слово токеном UNK, если его нет в нашем новом словаре\n",
    "processed_corpus = [[get_correct_words(tok) for tok in text] for text in tqdm(lemmas_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_unks(tokens):\n",
    "    \n",
    "    output_tokens = []\n",
    "    \n",
    "    for tok in tokens:\n",
    "        \n",
    "        if tok == 'UNK' and output_tokens and output_tokens[-1] == 'UNK':\n",
    "            continue\n",
    "            \n",
    "        output_tokens.append(tok)\n",
    "            \n",
    "    return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'думать далее милый барышня UNK UNK тинькоф звонить неделя'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['думать',\n",
       " 'далее',\n",
       " 'милый',\n",
       " 'барышня',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'тинькоф',\n",
       " 'звонить',\n",
       " 'неделя']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['думать', 'далее', 'милый', 'барышня', 'UNK', 'тинькоф', 'звонить', 'неделя']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_duplicate_unks(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:03<00:00, 367856.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# дедублируем подряд идущие унки (оставим только один)\n",
    "processed_corpus = [drop_duplicate_unks(sample) for sample in tqdm(processed_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Текстов с унками - 11.27 %'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_with_unk = [text for text in processed_corpus if 'UNK' in text]\n",
    "'Текстов с унками - {:.2f} %'.format(len(texts_with_unk) * 100 / len(processed_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "большой спасибо наш менеджер анне UNK кассир запомнить её имя качественный работа\n",
      "именно оформление соглашение условие предоставление кредит UNK взаимный право обязанность\n",
      "пойти разбираться отделение UNK банк добиться время отсылать citiphone\n",
      "UNK день ##число декабрь ##число мой доверие это железный ящик сильно пошатнуться\n",
      "заботиться мой деньга свой удерживать клиент UNK основание\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на тексты с унками\n",
    "for text in random.sample(texts_with_unk, k=5):\n",
    "    print(' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выглядит не так плохо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выберем подвыборку данных\n",
    "Чтобы быстрее выучить word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = processed_corpus[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processed_corpus.json', 'w') as f:\n",
    "    json.dump(sub_data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
